{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b13c4c15",
   "metadata": {
    "papermill": {
     "duration": 0.008378,
     "end_time": "2022-10-14T16:18:34.771499",
     "exception": false,
     "start_time": "2022-10-14T16:18:34.763121",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6bb836",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-14T16:18:34.787674Z",
     "iopub.status.busy": "2022-10-14T16:18:34.787119Z",
     "iopub.status.idle": "2022-10-14T16:18:37.275467Z",
     "shell.execute_reply": "2022-10-14T16:18:37.274516Z"
    },
    "papermill": {
     "duration": 2.499314,
     "end_time": "2022-10-14T16:18:37.278066",
     "exception": false,
     "start_time": "2022-10-14T16:18:34.778752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.preprocessing import label_binarize\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "import sys\n",
    "# sys.path.append('../input/rangerdeeplearningoptimizer/ranger')\n",
    "# from ranger import Ranger\n",
    "\n",
    "import warnings\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dced85b",
   "metadata": {
    "papermill": {
     "duration": 0.006923,
     "end_time": "2022-10-14T16:18:37.293015",
     "exception": false,
     "start_time": "2022-10-14T16:18:37.286092",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45da5a6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-14T16:18:37.308561Z",
     "iopub.status.busy": "2022-10-14T16:18:37.308060Z",
     "iopub.status.idle": "2022-10-14T16:18:37.374880Z",
     "shell.execute_reply": "2022-10-14T16:18:37.373999Z"
    },
    "papermill": {
     "duration": 0.076785,
     "end_time": "2022-10-14T16:18:37.376836",
     "exception": false,
     "start_time": "2022-10-14T16:18:37.300051",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = \"./\"\n",
    "feature_list = [\"smile\", \"target\", \"enzyme\"]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vector_size = 572*2\n",
    "\n",
    "# drop_out_rating = 0.3\n",
    "batch_size = 256\n",
    "learn_rating = 1e-3\n",
    "epo_num = 30\n",
    "weight_decay_rate = 1e-3\n",
    "cv = 5\n",
    "patience = 7\n",
    "delta = 0\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b67298",
   "metadata": {
    "id": "m425KHhNeKfD",
    "papermill": {
     "duration": 0.006757,
     "end_time": "2022-10-14T16:18:37.390736",
     "exception": false,
     "start_time": "2022-10-14T16:18:37.383979",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2916c91d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-14T16:18:37.408015Z",
     "iopub.status.busy": "2022-10-14T16:18:37.407046Z",
     "iopub.status.idle": "2022-10-14T16:18:37.417105Z",
     "shell.execute_reply": "2022-10-14T16:18:37.416219Z"
    },
    "id": "W3tUeTSteKfD",
    "papermill": {
     "duration": 0.020304,
     "end_time": "2022-10-14T16:18:37.419280",
     "exception": false,
     "start_time": "2022-10-14T16:18:37.398976",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model, path):\n",
    "        # print(\"val_loss={}\".format(val_loss))\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            # print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model, path):\n",
    "        if self.verbose:\n",
    "            print(f\"Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...\")\n",
    "        torch.save(model.state_dict(), path + \"/\" + \"model_checkpoint.pth\")\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a31d50c",
   "metadata": {
    "papermill": {
     "duration": 0.007398,
     "end_time": "2022-10-14T16:18:37.434165",
     "exception": false,
     "start_time": "2022-10-14T16:18:37.426767",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Ranger Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2e369a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-14T16:18:37.450709Z",
     "iopub.status.busy": "2022-10-14T16:18:37.450411Z",
     "iopub.status.idle": "2022-10-14T16:18:37.476804Z",
     "shell.execute_reply": "2022-10-14T16:18:37.475972Z"
    },
    "papermill": {
     "duration": 0.03727,
     "end_time": "2022-10-14T16:18:37.478942",
     "exception": false,
     "start_time": "2022-10-14T16:18:37.441672",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer\n",
    "from torch.optim.optimizer import Optimizer\n",
    "class Ranger(Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3,                       # lr\n",
    "                 alpha=0.5, k=6, N_sma_threshhold=5,           # Ranger options\n",
    "                 betas=(.95, 0.999), eps=1e-5, weight_decay=0,  # Adam options\n",
    "                 # Gradient centralization on or off, applied to conv layers only or conv + fc layers\n",
    "                 use_gc=True, gc_conv_only=False\n",
    "                 ):\n",
    "\n",
    "        # parameter checks\n",
    "        if not 0.0 <= alpha <= 1.0:\n",
    "            raise ValueError(f'Invalid slow update rate: {alpha}')\n",
    "        if not 1 <= k:\n",
    "            raise ValueError(f'Invalid lookahead steps: {k}')\n",
    "        if not lr > 0:\n",
    "            raise ValueError(f'Invalid Learning Rate: {lr}')\n",
    "        if not eps > 0:\n",
    "            raise ValueError(f'Invalid eps: {eps}')\n",
    "\n",
    "\n",
    "        # prep defaults and init torch.optim base\n",
    "        defaults = dict(lr=lr, alpha=alpha, k=k, step_counter=0, betas=betas,\n",
    "                        N_sma_threshhold=N_sma_threshhold, eps=eps, weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "        # adjustable threshold\n",
    "        self.N_sma_threshhold = N_sma_threshhold\n",
    "\n",
    "        # look ahead params\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.k = k\n",
    "\n",
    "        # radam buffer for state\n",
    "        self.radam_buffer = [[None, None, None] for ind in range(10)]\n",
    "\n",
    "        # gc on or off\n",
    "        self.use_gc = use_gc\n",
    "\n",
    "        # level of gradient centralization\n",
    "        self.gc_gradient_threshold = 3 if gc_conv_only else 1\n",
    "\n",
    "        print(\n",
    "            f\"Ranger optimizer loaded. \\nGradient Centralization usage = {self.use_gc}\")\n",
    "        if (self.use_gc and self.gc_gradient_threshold == 1):\n",
    "            print(f\"GC applied to both conv and fc layers\")\n",
    "        elif (self.use_gc and self.gc_gradient_threshold == 3):\n",
    "            print(f\"GC applied to conv layers only\")\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        print(\"set state called\")\n",
    "        super(Ranger, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "\n",
    "        # Evaluate averages and grad, update param tensors\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError(\n",
    "                        'Ranger optimizer does not support sparse gradients')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]  # get state dict for this param\n",
    "\n",
    "                if len(state) == 0:  \n",
    "\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "\n",
    "                    # look ahead weight storage now in state dict\n",
    "                    state['slow_buffer'] = torch.empty_like(p.data)\n",
    "                    state['slow_buffer'].copy_(p.data)\n",
    "\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(\n",
    "                        p_data_fp32)\n",
    "\n",
    "                # begin computations\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                # GC operation for Conv layers and FC layers\n",
    "                if grad.dim() > self.gc_gradient_threshold:\n",
    "                    grad.add_(-grad.mean(dim=tuple(range(1, grad.dim())), keepdim=True))\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                # compute variance mov avg\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                # compute mean moving avg\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "\n",
    "                buffered = self.radam_buffer[int(state['step'] % 10)]\n",
    "\n",
    "                if state['step'] == buffered[0]:\n",
    "                    N_sma, step_size = buffered[1], buffered[2]\n",
    "                else:\n",
    "                    buffered[0] = state['step']\n",
    "                    beta2_t = beta2 ** state['step']\n",
    "                    N_sma_max = 2 / (1 - beta2) - 1\n",
    "                    N_sma = N_sma_max - 2 * \\\n",
    "                        state['step'] * beta2_t / (1 - beta2_t)\n",
    "                    buffered[1] = N_sma\n",
    "                    if N_sma > self.N_sma_threshhold:\n",
    "                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (\n",
    "                            N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    else:\n",
    "                        step_size = 1.0 / (1 - beta1 ** state['step'])\n",
    "                    buffered[2] = step_size\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    p_data_fp32.add_(-group['weight_decay']\n",
    "                                     * group['lr'], p_data_fp32)\n",
    "\n",
    "                # apply lr\n",
    "                if N_sma > self.N_sma_threshhold:\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    p_data_fp32.addcdiv_(-step_size *\n",
    "                                         group['lr'], exp_avg, denom)\n",
    "                else:\n",
    "                    p_data_fp32.add_(-step_size * group['lr'], exp_avg)\n",
    "\n",
    "                p.data.copy_(p_data_fp32)\n",
    "\n",
    "                # integrated look ahead...\n",
    "                # we do it at the param level instead of group level\n",
    "                if state['step'] % group['k'] == 0:\n",
    "                    # get access to slow param tensor\n",
    "                    slow_p = state['slow_buffer']\n",
    "                    # (fast weights - slow weights) * alpha\n",
    "                    slow_p.add_(self.alpha, p.data - slow_p)\n",
    "                    # copy interpolated weights to RAdam param tensor\n",
    "                    p.data.copy_(slow_p)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337b44d4",
   "metadata": {
    "papermill": {
     "duration": 0.006929,
     "end_time": "2022-10-14T16:18:37.493037",
     "exception": false,
     "start_time": "2022-10-14T16:18:37.486108",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ffc0cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-14T16:18:37.508912Z",
     "iopub.status.busy": "2022-10-14T16:18:37.508037Z",
     "iopub.status.idle": "2022-10-14T16:18:37.649823Z",
     "shell.execute_reply": "2022-10-14T16:18:37.648864Z"
    },
    "papermill": {
     "duration": 0.152105,
     "end_time": "2022-10-14T16:18:37.652127",
     "exception": false,
     "start_time": "2022-10-14T16:18:37.500022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"./fusion_data/event.db\")\n",
    "# conn = sqlite3.connect(\"../input/ddidatasets/event.db\")\n",
    "df_drug = pd.read_sql(\"select * from drug;\", conn)\n",
    "extraction = pd.read_sql(\"select * from extraction;\", conn)\n",
    "mechanism = extraction[\"mechanism\"]\n",
    "action = extraction[\"action\"]\n",
    "drugA = extraction[\"drugA\"]\n",
    "drugB = extraction[\"drugB\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0839f2b5",
   "metadata": {
    "papermill": {
     "duration": 0.00686,
     "end_time": "2022-10-14T16:18:37.666693",
     "exception": false,
     "start_time": "2022-10-14T16:18:37.659833",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a90ac24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-14T16:18:37.682707Z",
     "iopub.status.busy": "2022-10-14T16:18:37.682368Z",
     "iopub.status.idle": "2022-10-14T16:18:37.698486Z",
     "shell.execute_reply": "2022-10-14T16:18:37.697502Z"
    },
    "papermill": {
     "duration": 0.026761,
     "end_time": "2022-10-14T16:18:37.700488",
     "exception": false,
     "start_time": "2022-10-14T16:18:37.673727",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare(df_drug, feature_list, mechanism, action, drugA, drugB):\n",
    "    d_label = {}\n",
    "    d_feature = {}\n",
    "\n",
    "    # Transfrom the interaction event to number\n",
    "    d_event = []\n",
    "    for i in range(len(mechanism)):\n",
    "        d_event.append(mechanism[i] + \" \" + action[i])\n",
    "\n",
    "    count = {}\n",
    "    for i in d_event:\n",
    "        if i in count:\n",
    "            count[i] += 1\n",
    "        else:\n",
    "            count[i] = 1\n",
    "    event_num = len(count)\n",
    "    list1 = sorted(count.items(), key=lambda x: x[1], reverse=True)\n",
    "    for i in range(len(list1)):\n",
    "        d_label[list1[i][0]] = i\n",
    "\n",
    "    vector = np.zeros(\n",
    "        (len(np.array(df_drug[\"name\"]).tolist()), 0), dtype=float\n",
    "    )  # vector=[]\n",
    "    for i in feature_list:\n",
    "        # vector = np.hstack((vector, feature_vector(i, df_drug, vector_size)))#1258*1258\n",
    "        tempvec = feature_vector(i, df_drug)\n",
    "        vector = np.hstack((vector, tempvec))\n",
    "    # Transfrom the drug ID to feature vector\n",
    "    for i in range(len(np.array(df_drug[\"name\"]).tolist())):\n",
    "        d_feature[np.array(df_drug[\"name\"]).tolist()[i]] = vector[i]\n",
    "\n",
    "    # Use the dictionary to obtain feature vector and label\n",
    "    new_feature = []\n",
    "    new_label = []\n",
    "\n",
    "    for i in range(len(d_event)):\n",
    "        temp = np.hstack((d_feature[drugA[i]], d_feature[drugB[i]]))\n",
    "        new_feature.append(temp)\n",
    "        new_label.append(d_label[d_event[i]])\n",
    "\n",
    "    new_feature = np.array(new_feature)  # 323539*....\n",
    "    new_label = np.array(new_label)  # 323539\n",
    "\n",
    "    return new_feature, new_label, event_num\n",
    "\n",
    "\n",
    "def feature_vector(feature_name, df):\n",
    "    def Jaccard(matrix):\n",
    "        matrix = np.mat(matrix)\n",
    "\n",
    "        numerator = matrix * matrix.T\n",
    "\n",
    "        denominator = (\n",
    "            np.ones(np.shape(matrix)) * matrix.T\n",
    "            + matrix * np.ones(np.shape(matrix.T))\n",
    "            - matrix * matrix.T\n",
    "        )\n",
    "\n",
    "        return numerator / denominator\n",
    "\n",
    "    all_feature = []\n",
    "    drug_list = np.array(df[feature_name]).tolist()\n",
    "    # Features for each drug, for example, when feature_name is target, drug_list=[\"P30556|P05412\",\"P28223|P46098|……\"]\n",
    "    for i in drug_list:\n",
    "        for each_feature in i.split(\"|\"):\n",
    "            if each_feature not in all_feature:\n",
    "                all_feature.append(each_feature)  # obtain all the features\n",
    "    feature_matrix = np.zeros((len(drug_list), len(all_feature)), dtype=float)\n",
    "    df_feature = DataFrame(\n",
    "        feature_matrix, columns=all_feature\n",
    "    )  # Consrtuct feature matrices with key of dataframe\n",
    "    for i in range(len(drug_list)):\n",
    "        for each_feature in df[feature_name].iloc[i].split(\"|\"):\n",
    "            df_feature[each_feature].iloc[i] = 1\n",
    "\n",
    "    df_feature = np.array(df_feature)\n",
    "    sim_matrix = np.array(Jaccard(df_feature))\n",
    "\n",
    "    print(feature_name + \" len is:\" + str(len(sim_matrix[0])))\n",
    "    return sim_matrix\n",
    "\n",
    "\n",
    "class DDIDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.len = len(x)\n",
    "        self.x_data = torch.from_numpy(x)\n",
    "\n",
    "        self.y_data = torch.from_numpy(y)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45febc4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-14T16:18:37.717641Z",
     "iopub.status.busy": "2022-10-14T16:18:37.716170Z",
     "iopub.status.idle": "2022-10-14T16:18:47.021489Z",
     "shell.execute_reply": "2022-10-14T16:18:47.019966Z"
    },
    "papermill": {
     "duration": 9.316509,
     "end_time": "2022-10-14T16:18:47.024713",
     "exception": false,
     "start_time": "2022-10-14T16:18:37.708204",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_feature, new_label, event_num = prepare(\n",
    "    df_drug, feature_list, mechanism, action, drugA, drugB\n",
    ")\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(new_feature)\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(new_label)\n",
    "print(\"dataset len\", len(new_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd3cb83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-14T16:18:47.041473Z",
     "iopub.status.busy": "2022-10-14T16:18:47.041158Z",
     "iopub.status.idle": "2022-10-14T16:18:47.049173Z",
     "shell.execute_reply": "2022-10-14T16:18:47.048147Z"
    },
    "papermill": {
     "duration": 0.019903,
     "end_time": "2022-10-14T16:18:47.052739",
     "exception": false,
     "start_time": "2022-10-14T16:18:47.032836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_feature.shape, new_label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25419fa5",
   "metadata": {
    "papermill": {
     "duration": 0.007085,
     "end_time": "2022-10-14T16:18:47.067096",
     "exception": false,
     "start_time": "2022-10-14T16:18:47.060011",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9521cb46",
   "metadata": {
    "papermill": {
     "duration": 0.007333,
     "end_time": "2022-10-14T16:18:47.155626",
     "exception": false,
     "start_time": "2022-10-14T16:18:47.148293",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## CNN-Siam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b517bf2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-14T16:18:47.171925Z",
     "iopub.status.busy": "2022-10-14T16:18:47.171104Z",
     "iopub.status.idle": "2022-10-14T16:18:47.182769Z",
     "shell.execute_reply": "2022-10-14T16:18:47.181922Z"
    },
    "papermill": {
     "duration": 0.021852,
     "end_time": "2022-10-14T16:18:47.184885",
     "exception": false,
     "start_time": "2022-10-14T16:18:47.163033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CNN_Siam(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_Siam, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(3, 64, 3, bias=False)\n",
    "        # self.BN1 = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(64, 128, 3, bias=False)\n",
    "        # self.BN2 = nn.BatchNorm1d(128)\n",
    "\n",
    "        self.conv3_1 = nn.Conv1d(128, 128, 3, padding=1, bias=False)\n",
    "        # self.BN3_1 = nn.BatchNorm1d(128)\n",
    "        self.conv3_2 = nn.Conv1d(128, 128, 3, padding=1, bias=False)\n",
    "        # self.BN3_2 = nn.BatchNorm1d(128)\n",
    "\n",
    "        self.conv4 = nn.Conv1d(128, 256, 3, bias=False)\n",
    "        self.BN4 = nn.BatchNorm1d(256)\n",
    "\n",
    "        self.fc =  nn.Sequential(\n",
    "            nn.Linear(256*283, 2048),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(2048, 256),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(256, event_num),\n",
    "        )\n",
    "    \n",
    "    def encode(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        res_bef = F.relu(self.conv2(x))\n",
    "        # print(f'before: {res_bef.shape}')\n",
    "\n",
    "        x = F.relu(self.conv3_1(res_bef))\n",
    "        # print(f'after1: {x.shape}')\n",
    "\n",
    "        res_aft = F.relu(self.conv3_2(x))\n",
    "        # print(f'after2: {x.shape}')\n",
    "        \n",
    "        # residual connection\n",
    "        x = res_aft + res_bef\n",
    "        x = F.relu(self.BN4(self.conv4(x)))\n",
    "        x = F.max_pool1d(x, 2)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, 3, vector_size)\n",
    "        x1 = x[:, :, :vector_size // 2]\n",
    "        x2 = x[:, :, vector_size // 2:]\n",
    "        x1 = self.encode(x1)\n",
    "        x2 = self.encode(x2)\n",
    "        \n",
    "        # 在通道维度上拼接\n",
    "        # x = torch.cat((x1, x2), dim=1)\n",
    "        \n",
    "        # 将embedding相加\n",
    "        x = x1 + x2\n",
    "        # print(f'x: {x.shape}')\n",
    "\n",
    "        x = x.reshape(-1, 256*283)\n",
    "        y = self.fc(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e9e903",
   "metadata": {
    "papermill": {
     "duration": 0.00713,
     "end_time": "2022-10-14T16:18:47.199255",
     "exception": false,
     "start_time": "2022-10-14T16:18:47.192125",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "loss fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0405a7c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-14T16:18:47.215360Z",
     "iopub.status.busy": "2022-10-14T16:18:47.214608Z",
     "iopub.status.idle": "2022-10-14T16:18:47.222497Z",
     "shell.execute_reply": "2022-10-14T16:18:47.221689Z"
    },
    "papermill": {
     "duration": 0.017948,
     "end_time": "2022-10-14T16:18:47.224473",
     "exception": false,
     "start_time": "2022-10-14T16:18:47.206525",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class focal_loss(nn.Module):\n",
    "    def __init__(self, gamma=2):\n",
    "        super(focal_loss, self).__init__()\n",
    "\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, preds, labels):\n",
    "        # assert preds.dim() == 2 and labels.dim()==1\n",
    "        labels = labels.view(-1, 1).type(torch.int64)  # [B * S, 1]\n",
    "        preds = preds.view(-1, preds.size(-1))  # [B * S, C]\n",
    "\n",
    "        preds_logsoft = F.log_softmax(preds, dim=1)  # 先softmax, 然后取log\n",
    "        preds_softmax = torch.exp(preds_logsoft)  # softmax\n",
    "\n",
    "        preds_softmax = preds_softmax.gather(1, labels)  # 这部分实现nll_loss ( crossempty = log_softmax + nll )\n",
    "        preds_logsoft = preds_logsoft.gather(1, labels)\n",
    "\n",
    "        loss = -torch.mul(torch.pow((1 - preds_softmax), self.gamma),\n",
    "                          preds_logsoft)  # torch.pow((1-preds_softmax), self.gamma) 为focal loss中 (1-pt)**γ\n",
    "\n",
    "        loss = loss.mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "# def mixup(x1, x2, y1, y2, alpha):\n",
    "#     beta = np.random.beta(alpha, alpha)\n",
    "#     x = beta * x1 + (1 - beta) * x2\n",
    "#     y = beta * y1 + (1 - beta) * y2\n",
    "#     return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaced25",
   "metadata": {
    "papermill": {
     "duration": 0.007168,
     "end_time": "2022-10-14T16:18:47.238952",
     "exception": false,
     "start_time": "2022-10-14T16:18:47.231784",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TEST PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a4d144",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-14T16:18:47.255249Z",
     "iopub.status.busy": "2022-10-14T16:18:47.254434Z",
     "iopub.status.idle": "2022-10-14T16:18:47.258547Z",
     "shell.execute_reply": "2022-10-14T16:18:47.257723Z"
    },
    "papermill": {
     "duration": 0.014292,
     "end_time": "2022-10-14T16:18:47.260508",
     "exception": false,
     "start_time": "2022-10-14T16:18:47.246216",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# inp = torch.randn(2, 1144*3)\n",
    "# mod = CNN_Siam()\n",
    "# mod(inp).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3951255",
   "metadata": {
    "papermill": {
     "duration": 0.007079,
     "end_time": "2022-10-14T16:18:47.275069",
     "exception": false,
     "start_time": "2022-10-14T16:18:47.267990",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d512be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-14T16:18:47.291576Z",
     "iopub.status.busy": "2022-10-14T16:18:47.291288Z",
     "iopub.status.idle": "2022-10-14T16:18:47.308523Z",
     "shell.execute_reply": "2022-10-14T16:18:47.307640Z"
    },
    "papermill": {
     "duration": 0.028082,
     "end_time": "2022-10-14T16:18:47.310473",
     "exception": false,
     "start_time": "2022-10-14T16:18:47.282391",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "train_epochs_loss = []\n",
    "valid_epochs_loss = []\n",
    "\n",
    "def train_fn(model, x_train, y_train, x_test, y_test, event_num):\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=learn_rating, weight_decay=weight_decay_rate)\n",
    "    # optimizer = optim.RAdam(model.parameters(), lr=learn_rating, weight_decay=weight_decay_rate)\n",
    "\n",
    "    # Ranger(RAdam+LookAhead) + CosineAnnealingLR: 50epochs一个半周期\n",
    "    optimizer = Ranger(model.parameters(), lr=learn_rating, weight_decay=weight_decay_rate, betas=(0.95, 0.999), eps=1e-6)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=epo_num, eta_min=1e-6)\n",
    "\n",
    "\n",
    "    my_loss = focal_loss()\n",
    "    model = model.to(device)\n",
    "    earlystop = EarlyStopping(patience=patience, delta=delta)\n",
    "\n",
    "    x_train = np.vstack(\n",
    "        (\n",
    "            x_train,\n",
    "            np.hstack(\n",
    "                (x_train[:, len(x_train[0]) // 2 :], x_train[:, : len(x_train[0]) // 2])\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    y_train = np.hstack((y_train, y_train))\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(x_train)\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(y_train)\n",
    "\n",
    "    len_train = len(y_train)\n",
    "    len_test = len(y_test)\n",
    "    print(\"arg train len\", len(y_train))\n",
    "    print(\"test len\", len(y_test))\n",
    "\n",
    "    train_dataset = DDIDataset(x_train, np.array(y_train))\n",
    "    test_dataset = DDIDataset(x_test, np.array(y_test))\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    for epoch in range(epo_num):\n",
    "\n",
    "        running_loss = 0.0\n",
    "\n",
    "        model.train()\n",
    "        for batch_idx, data in enumerate(train_loader, 0):\n",
    "            x, y = data\n",
    "\n",
    "            # mixup\n",
    "            lam = np.random.beta(0.5, 0.5)\n",
    "            index = torch.randperm(x.size()[0])\n",
    "            inputs = lam * x + (1 - lam) * x[index, :]\n",
    "\n",
    "            targets_a, targets_b = y, y[index]\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            targets_a = targets_a.to(device)\n",
    "            targets_b = targets_b.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            # forward + backward+update\n",
    "            pred = model(inputs.float())\n",
    "\n",
    "            loss = lam * my_loss(pred, targets_a) + (1 - lam) * my_loss(pred, targets_b)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        testing_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, data in enumerate(test_loader, 0):\n",
    "                inputs, target = data\n",
    "\n",
    "                inputs = inputs.to(device)\n",
    "\n",
    "                target = target.to(device)\n",
    "\n",
    "                pred = model(inputs.float())\n",
    "\n",
    "                loss = my_loss(pred, target)\n",
    "                testing_loss += loss.item()\n",
    "        # if epoch+1 % 5 == 0:\n",
    "        print(\n",
    "            \"epoch [%d] loss: %.6f testing_loss: %.6f \"\n",
    "            % (epoch + 1, running_loss / len_train, testing_loss / len_test)\n",
    "        )\n",
    "        \n",
    "        train_epochs_loss.append(running_loss / len_train)\n",
    "        valid_epochs_loss.append(testing_loss / len_test)\n",
    "        earlystop(valid_epochs_loss[-1], model, file_path)\n",
    "        if earlystop.early_stop:\n",
    "            print(\"Early stopping\\n\")\n",
    "            break\n",
    "\n",
    "\n",
    "    pre_score = np.zeros((0, event_num), dtype=float)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(test_loader, 0):\n",
    "            inputs, _ = data\n",
    "            inputs = inputs.to(device)\n",
    "            X = model(inputs.float())\n",
    "            pre_score = np.vstack((pre_score, F.softmax(X).cpu().numpy()))\n",
    "    return pre_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee22ab06",
   "metadata": {
    "papermill": {
     "duration": 0.007065,
     "end_time": "2022-10-14T16:18:47.324850",
     "exception": false,
     "start_time": "2022-10-14T16:18:47.317785",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Evaluation fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7315bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-14T16:18:47.341152Z",
     "iopub.status.busy": "2022-10-14T16:18:47.340885Z",
     "iopub.status.idle": "2022-10-14T16:18:47.360340Z",
     "shell.execute_reply": "2022-10-14T16:18:47.359514Z"
    },
    "papermill": {
     "duration": 0.030182,
     "end_time": "2022-10-14T16:18:47.362290",
     "exception": false,
     "start_time": "2022-10-14T16:18:47.332108",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def roc_aupr_score(y_true, y_score, average=\"macro\"):\n",
    "    def _binary_roc_aupr_score(y_true, y_score):\n",
    "        precision, recall, pr_thresholds = precision_recall_curve(y_true, y_score)\n",
    "        return auc(recall, precision)\n",
    "\n",
    "    def _average_binary_score(\n",
    "        binary_metric, y_true, y_score, average\n",
    "    ):  # y_true= y_one_hot\n",
    "        if average == \"binary\":\n",
    "            return binary_metric(y_true, y_score)\n",
    "        if average == \"micro\":\n",
    "            y_true = y_true.ravel()\n",
    "            y_score = y_score.ravel()\n",
    "        if y_true.ndim == 1:\n",
    "            y_true = y_true.reshape((-1, 1))\n",
    "        if y_score.ndim == 1:\n",
    "            y_score = y_score.reshape((-1, 1))\n",
    "        n_classes = y_score.shape[1]\n",
    "        score = np.zeros((n_classes,))\n",
    "        for c in range(n_classes):\n",
    "            y_true_c = y_true.take([c], axis=1).ravel()\n",
    "            y_score_c = y_score.take([c], axis=1).ravel()\n",
    "            score[c] = binary_metric(y_true_c, y_score_c)\n",
    "        return np.average(score)\n",
    "\n",
    "    return _average_binary_score(_binary_roc_aupr_score, y_true, y_score, average)\n",
    "\n",
    "\n",
    "def evaluate(pred_type, pred_score, y_test, event_num):\n",
    "    all_eval_type = 11\n",
    "    result_all = np.zeros((all_eval_type, 1), dtype=float)\n",
    "    each_eval_type = 6\n",
    "    result_eve = np.zeros((event_num, each_eval_type), dtype=float)\n",
    "    y_one_hot = label_binarize(y_test, classes=range(event_num))\n",
    "    pred_one_hot = label_binarize(pred_type, classes=range(event_num))\n",
    "    result_all[0] = accuracy_score(y_test, pred_type)\n",
    "    result_all[1] = roc_aupr_score(y_one_hot, pred_score, average=\"micro\")\n",
    "    result_all[2] = roc_aupr_score(y_one_hot, pred_score, average=\"macro\")\n",
    "    result_all[3] = roc_auc_score(y_one_hot, pred_score, average=\"micro\")\n",
    "    result_all[4] = roc_auc_score(y_one_hot, pred_score, average=\"macro\")\n",
    "    result_all[5] = f1_score(y_test, pred_type, average=\"micro\")\n",
    "    result_all[6] = f1_score(y_test, pred_type, average=\"macro\")\n",
    "    result_all[7] = precision_score(y_test, pred_type, average=\"micro\")\n",
    "    result_all[8] = precision_score(y_test, pred_type, average=\"macro\")\n",
    "    result_all[9] = recall_score(y_test, pred_type, average=\"micro\")\n",
    "    result_all[10] = recall_score(y_test, pred_type, average=\"macro\")\n",
    "    for i in range(event_num):\n",
    "        result_eve[i, 0] = accuracy_score(\n",
    "            y_one_hot.take([i], axis=1).ravel(), pred_one_hot.take([i], axis=1).ravel()\n",
    "        )\n",
    "        result_eve[i, 1] = roc_aupr_score(\n",
    "            y_one_hot.take([i], axis=1).ravel(),\n",
    "            pred_one_hot.take([i], axis=1).ravel(),\n",
    "            average=None,\n",
    "        )\n",
    "        result_eve[i, 2] = roc_auc_score(\n",
    "            y_one_hot.take([i], axis=1).ravel(),\n",
    "            pred_one_hot.take([i], axis=1).ravel(),\n",
    "            average=None,\n",
    "        )\n",
    "        result_eve[i, 3] = f1_score(\n",
    "            y_one_hot.take([i], axis=1).ravel(),\n",
    "            pred_one_hot.take([i], axis=1).ravel(),\n",
    "            average=\"binary\",\n",
    "        )\n",
    "        result_eve[i, 4] = precision_score(\n",
    "            y_one_hot.take([i], axis=1).ravel(),\n",
    "            pred_one_hot.take([i], axis=1).ravel(),\n",
    "            average=\"binary\",\n",
    "        )\n",
    "        result_eve[i, 5] = recall_score(\n",
    "            y_one_hot.take([i], axis=1).ravel(),\n",
    "            pred_one_hot.take([i], axis=1).ravel(),\n",
    "            average=\"binary\",\n",
    "        )\n",
    "    return [result_all, result_eve]\n",
    "\n",
    "\n",
    "def save_result(filepath, result_type, result):\n",
    "\n",
    "    index = ['accuracy', 'aupr_micro', 'aupr_macro', 'auc_micro', 'auc_macro', 'f1_micro', 'f1_macro', 'precision_micro', 'precision_macro', 'recall_micro', 'recall_macro']\n",
    "\n",
    "    os.makedirs(filepath, exist_ok=True)\n",
    "    if result_type == 'all':\n",
    "        all_ = pd.DataFrame(result, index=index)\n",
    "        all_.to_csv(f'{file_path}/results_all.csv')\n",
    "    else:\n",
    "        each = pd.DataFrame(result)\n",
    "        each.to_csv(f'{file_path}/results_each.csv', index=False)\n",
    "#         each.columns = ['accuracy', 'aupr', 'auc', 'f1', 'precision', 'recall']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac253ecd",
   "metadata": {
    "papermill": {
     "duration": 0.007111,
     "end_time": "2022-10-14T16:18:47.376673",
     "exception": false,
     "start_time": "2022-10-14T16:18:47.369562",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235f972c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-14T16:18:47.392794Z",
     "iopub.status.busy": "2022-10-14T16:18:47.392474Z",
     "iopub.status.idle": "2022-10-14T16:18:47.400469Z",
     "shell.execute_reply": "2022-10-14T16:18:47.399445Z"
    },
    "papermill": {
     "duration": 0.018505,
     "end_time": "2022-10-14T16:18:47.402498",
     "exception": false,
     "start_time": "2022-10-14T16:18:47.383993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cross_val(feature, label, event_num):\n",
    "    skf = StratifiedKFold(n_splits=cv)\n",
    "    y_true = np.array([])\n",
    "    y_score = np.zeros((0, event_num), dtype=float)\n",
    "    y_pred = np.array([])\n",
    "    model = CNN_Siam()\n",
    "    \n",
    "    cv_results = []\n",
    "\n",
    "    for train_index, test_index in skf.split(feature, label):\n",
    "\n",
    "        X_train, X_test = feature[train_index], feature[test_index]\n",
    "        y_train, y_test = label[train_index], label[test_index]\n",
    "        print(\"train len\", len(y_train))\n",
    "        print(\"test len\", len(y_test))\n",
    "\n",
    "        pred_score = train_fn(model, X_train, y_train, X_test, y_test, event_num)\n",
    "\n",
    "        pred_type = np.argmax(pred_score, axis=1)\n",
    "        y_pred = np.hstack((y_pred, pred_type))\n",
    "        y_score = np.row_stack((y_score, pred_score))\n",
    "        y_true = np.hstack((y_true, y_test))\n",
    "\n",
    "        cv_results.append(evaluate(y_pred, y_score, y_true, event_num))\n",
    "\n",
    "    print(\"Training finished!\")\n",
    "\n",
    "    return cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50af63a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-14T16:18:47.419688Z",
     "iopub.status.busy": "2022-10-14T16:18:47.419341Z",
     "iopub.status.idle": "2022-10-14T18:24:51.249689Z",
     "shell.execute_reply": "2022-10-14T18:24:51.247703Z"
    },
    "papermill": {
     "duration": 7563.841421,
     "end_time": "2022-10-14T18:24:51.251926",
     "exception": false,
     "start_time": "2022-10-14T16:18:47.410505",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cv_results = cross_val(new_feature, new_label, event_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df182f8",
   "metadata": {
    "papermill": {
     "duration": 0.015993,
     "end_time": "2022-10-14T18:24:51.284979",
     "exception": false,
     "start_time": "2022-10-14T18:24:51.268986",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697a0024",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-14T18:24:51.319685Z",
     "iopub.status.busy": "2022-10-14T18:24:51.318989Z",
     "iopub.status.idle": "2022-10-14T18:24:51.847041Z",
     "shell.execute_reply": "2022-10-14T18:24:51.846087Z"
    },
    "papermill": {
     "duration": 0.549388,
     "end_time": "2022-10-14T18:24:51.850641",
     "exception": false,
     "start_time": "2022-10-14T18:24:51.301253",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "plt.plot(train_epochs_loss, label=\"train_loss\")\n",
    "plt.plot(valid_epochs_loss, label=\"valid_loss\")\n",
    "plt.title(\"epochs_loss\")\n",
    "plt.legend()\n",
    "# plt.savefig(\"./Tmax100-loss.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33001c0",
   "metadata": {
    "papermill": {
     "duration": 0.027187,
     "end_time": "2022-10-14T18:24:51.904244",
     "exception": false,
     "start_time": "2022-10-14T18:24:51.877057",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50610ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_res = [res[0] for res in cv_results]\n",
    "all_res = np.array(all_res).reshape(11, 5)\n",
    "index = ['accuracy', 'aupr_micro', 'aupr_macro', 'auc_micro', 'auc_macro', 'f1_micro', 'f1_macro', 'precision_micro', 'precision_macro', 'recall_micro', 'recall_macro']\n",
    "all_ = pd.DataFrame(all_res, index=index).T\n",
    "all_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f821b18c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-14T18:24:51.957623Z",
     "iopub.status.busy": "2022-10-14T18:24:51.957276Z",
     "iopub.status.idle": "2022-10-14T18:24:51.963275Z",
     "shell.execute_reply": "2022-10-14T18:24:51.962229Z"
    },
    "papermill": {
     "duration": 0.027714,
     "end_time": "2022-10-14T18:24:51.965575",
     "exception": false,
     "start_time": "2022-10-14T18:24:51.937861",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# index = ['accuracy', 'aupr_micro', 'aupr_macro', 'auc_micro', 'auc_macro', 'f1_micro', 'f1_macro', 'precision_micro', 'precision_macro', 'recall_micro', 'recall_macro']\n",
    "# all_ = pd.DataFrame(result_all, index=index)\n",
    "# each = pd.DataFrame(result_eve)\n",
    "# each.columns = ['accuracy', 'aupr', 'auc', 'f1', 'precision', 'recall']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7586.724292,
   "end_time": "2022-10-14T18:24:53.675851",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-10-14T16:18:26.951559",
   "version": "2.3.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "1ba9967f3e51e7e4679974bf13927c3fdc0d25abe87ee9098dffdbe1377e3e37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
